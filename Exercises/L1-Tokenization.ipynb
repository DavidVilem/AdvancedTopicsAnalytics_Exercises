{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R36i0tfEz4Vl"
      },
      "source": [
        "<center>\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=\"../Images/javeriana.PNG\" width=\"800\" height=\"600\">\n",
        "</div>\n",
        "\n",
        "**Juan David Villate Lemus**\n",
        "\n",
        "**José Rafael Peña Gutiérrez**\n",
        "\n",
        "**Laura Katherine Moreno Giraldo**\n",
        "\n",
        "**William Ricardo Fernández Garnica**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nfb0e7Js1dEv"
      },
      "source": [
        "### Tokenización\n",
        "\n",
        "En NLP el proceso de convertir nuestras secuencias de caracteres, palabras o párrafos en inputs para la computadora se llama **tokenización**. Se puede pensar al token como la unidad para procesamiento semántico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iv7vRwTzzXvH",
        "outputId": "7fa4e32f-aa71-4514-82c1-aaa667e22593"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['¿Cuánto', 'tiempo', 'pasó', 'desde', 'que', 'comí', 'una', 'manzana?']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "tk = WhitespaceTokenizer()\n",
        "texto = \"¿Cuánto tiempo pasó desde que comí una manzana?\"\n",
        "texto_tokenizado = tk.tokenize(texto)\n",
        "print(texto_tokenizado)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbM9INzk3HWO"
      },
      "source": [
        "como vemos *manzana* y *cuánto* figuran con el signo de pregunta. Y si tuvieramos la palabra manzana mencionada otra vez saldría nuevamente como un token diferente. Lo mismo nos sucedería si aparece una coma o un punto ¿Cómo hacemos para evitarlo?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR4uGVu73kgW"
      },
      "source": [
        "Podemos utilizar `TreebankWordTokenizer` en lugar de `WhitespaceTokenizer`. También podemos preprocesar el texto quitando comas y signos de puntuación y separar por espacios, o bien utilizar opciones como `WordPunctTokenizer` que separa por palabras tomando como separadores todo lo que no sea un caracter alfabetico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woH8Vq5W1bwp",
        "outputId": "cd55be02-9f9a-4d2c-971d-c53603f6b587"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "texto = \"¿Cuánto tiempo pasó desde que comí una manzana?\"\n",
        "texto_tokenizado1 = WordPunctTokenizer().tokenize(texto)\n",
        "texto_tokenizado2 = TreebankWordTokenizer().tokenize(texto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6D2WNuV18eX",
        "outputId": "2d975450-e427-4f41-9476-f573dc6d2d36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['¿', 'Cuánto', 'tiempo', 'pasó', 'desde', 'que', 'comí', 'una', 'manzana', '?']\n"
          ]
        }
      ],
      "source": [
        "print(texto_tokenizado1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAMX05Gx1_qW",
        "outputId": "de4e10b0-183d-4cf6-f65d-840df669f1ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['¿Cuánto', 'tiempo', 'pasó', 'desde', 'que', 'comí', 'una', 'manzana', '?']\n"
          ]
        }
      ],
      "source": [
        "print(texto_tokenizado2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi0PPMN038Qc"
      },
      "source": [
        "Como evidenciamos la opción de `TreebankWordTokenizer` es la más popular en inglés el signo de apertura de pregunta \"¿\" fue un problema para ella. Mientras que la opción de WordPunctTokenizer no tuvo ningún problema."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Respuesta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpqVpFFY4gxJ"
      },
      "source": [
        "¿cómo crees que se comportarían frente a una apóstrofe?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "i3LHqWjY2ATa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['I', 'can', \"'\", 't', 'believe', 'it', \"'\", 's', 'already', 'noon'],\n",
              " ['I', 'ca', \"n't\", 'believe', 'it', \"'s\", 'already', 'noon'])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Texto con apóstrofe\n",
        "texto_apostrofe = \"I can't believe it's already noon\"\n",
        "\n",
        "# Tokenizar el texto usando WordPunctTokenizer\n",
        "texto_apostrofe_tokenizado1 = WordPunctTokenizer().tokenize(texto_apostrofe)\n",
        "\n",
        "# Tokenizar el texto usando TreebankWordTokenizer\n",
        "texto_apostrofe_tokenizado2 = TreebankWordTokenizer().tokenize(texto_apostrofe)\n",
        "\n",
        "# Imprimir los resultados\n",
        "texto_apostrofe_tokenizado1, texto_apostrofe_tokenizado2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Con WordPunctTokenizer: La frase se divide en ['I', 'can', \"'\", 't', 'believe', 'it', \"'\", 's', 'already', 'noon']. Este tokenizador separa las apóstrofes del resto de la palabra, tratándolas como tokens independientes. Por lo tanto, las contracciones como \"can't\" y \"it's\" se dividen en partes: \"can\" y \"'t\" para \"can't\", \"it\" y \"'s\" para \"it's\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Con TreebankWordTokenizer: La frase se tokeniza como ['I', 'ca', \"n't\", 'believe', 'it', \"'s\", 'already', 'noon']. Este enfoque muestra cómo el TreebankWordTokenizer maneja las contracciones de manera más precisa, manteniendo la negación \"n't\" como un token separado y correctamente asociado con \"can't\", y haciendo algo similar con \"'s\" en \"it's\". Este comportamiento refleja un manejo más sofisticado de las apóstrofes, más adecuado para el procesamiento de textos en inglés, donde las contracciones son comunes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Otra Aproximacion, seria validar una limpieza previa\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Texto Limpio:     I cannot believe it is already noon\n",
            "Texto WordPunctTokenizer:     ['I', 'cannot', 'believe', 'it', 'is', 'already', 'noon']\n",
            "Texto TreebankWordTokenizer:     ['I', 'can', 'not', 'believe', 'it', 'is', 'already', 'noon']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Función para reemplazar contracciones comunes por sus formas extendidas\n",
        "def reemplazar_contracciones(texto):\n",
        "    # Diccionario de reemplazos\n",
        "    reemplazos = {\n",
        "        \"can't\": \"cannot\",\n",
        "        \"it's\": \"it is\",\n",
        "        \"'re\": \" are\",\n",
        "        \"n't\": \" not\",\n",
        "        \"'s\": \" is\",\n",
        "        \"'m\": \" am\",\n",
        "        \"'ll\": \" will\",\n",
        "        \"'d\": \" would\",\n",
        "        \"'ve\": \" have\"\n",
        "    }\n",
        "    \n",
        "    # Realizar reemplazos\n",
        "    for contraccion, reemplazo in reemplazos.items():\n",
        "        texto = re.sub(contraccion, reemplazo, texto)\n",
        "        \n",
        "    return texto\n",
        "\n",
        "# Texto original con apóstrofes\n",
        "texto_apostrofe_original = \"I can't believe it's already noon\"\n",
        "\n",
        "# Reemplazar contracciones\n",
        "texto_sin_contracciones = reemplazar_contracciones(texto_apostrofe_original)\n",
        "\n",
        "# Tokenizar el texto limpio usando ambos tokenizadores\n",
        "texto_tokenizado_sin_contracciones1 = WordPunctTokenizer().tokenize(texto_sin_contracciones)\n",
        "texto_tokenizado_sin_contracciones2 = TreebankWordTokenizer().tokenize(texto_sin_contracciones)\n",
        "\n",
        "print('Texto Limpio:    ',texto_sin_contracciones)\n",
        "print('Texto WordPunctTokenizer:    ',texto_tokenizado_sin_contracciones1)\n",
        "print('Texto TreebankWordTokenizer:    ',texto_tokenizado_sin_contracciones2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Con WordPunctTokenizer: La frase se tokeniza como ['I', 'cannot', 'believe', 'it', 'is', 'already', 'noon']. Este tokenizador maneja el texto limpio de manera efectiva, sin separar incorrectamente las palabras debido a las apóstrofes.\n",
        "\n",
        "Con TreebankWordTokenizer: La frase se tokeniza como ['I', 'can', 'not', 'believe', 'it', 'is', 'already', 'noon']. Interesantemente, este tokenizador separa \"cannot\" en \"can\" y \"not\", lo cual es una peculiaridad de cómo maneja ciertas palabras, incluso después de la normalización."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNttIjEOIIIusECPM29Dzrd",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
